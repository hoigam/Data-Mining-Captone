{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://d396qusza40orc.cloudfront.net/dataminingcapstone/Task6/Hygiene.tar.gz\n    \n!tar -xvf /kaggle/working/Hygiene.tar.gz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-19T05:55:24.074021Z","iopub.execute_input":"2024-04-19T05:55:24.075342Z","iopub.status.idle":"2024-04-19T05:55:28.749833Z","shell.execute_reply.started":"2024-04-19T05:55:24.075279Z","shell.execute_reply":"2024-04-19T05:55:28.748318Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2024-04-19 05:55:25--  https://d396qusza40orc.cloudfront.net/dataminingcapstone/Task6/Hygiene.tar.gz\nResolving d396qusza40orc.cloudfront.net (d396qusza40orc.cloudfront.net)... 52.84.160.76, 52.84.160.159, 52.84.160.182, ...\nConnecting to d396qusza40orc.cloudfront.net (d396qusza40orc.cloudfront.net)|52.84.160.76|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 39134299 (37M) [application/x-gzip]\nSaving to: 'Hygiene.tar.gz'\n\nHygiene.tar.gz      100%[===================>]  37.32M  66.0MB/s    in 0.6s    \n\n2024-04-19 05:55:26 (66.0 MB/s) - 'Hygiene.tar.gz' saved [39134299/39134299]\n\nHygiene/\nHygiene/hygiene.dat.additional\nHygiene/hygiene.dat\nHygiene/hygiene.dat.labels\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport spacy\nimport nltk\n\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T05:55:28.752883Z","iopub.execute_input":"2024-04-19T05:55:28.753969Z","iopub.status.idle":"2024-04-19T05:55:38.177392Z","shell.execute_reply.started":"2024-04-19T05:55:28.753889Z","shell.execute_reply":"2024-04-19T05:55:38.175833Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Summarization:\n\n## Text Preprocessing\n- normalization: lowercase, lemmatization\n- removal: irrelevant characters(x): stop_words include 'not'\n- tokenization: N-gram(2-4)\n\n## Feature engineering\n+ TF-IDF(uni-gram, bi-gram)\n+ Word embedding[Glove]\n+ feature selection(to do)\n\n## Model selections\n+ Logistic regression\n+ random forest\n+ SVM\n\n## Criteria:\n+ CV\n+ roc_auc_score\n+ F1_score","metadata":{}},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{}},{"cell_type":"code","source":"base_path = '/kaggle/working/Hygiene'\n\nreviews_path = os.path.join(base_path, 'hygiene.dat')\nlabels_path = os.path.join(base_path, 'hygiene.dat.labels')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T05:55:38.179177Z","iopub.execute_input":"2024-04-19T05:55:38.179884Z","iopub.status.idle":"2024-04-19T05:55:38.186723Z","shell.execute_reply.started":"2024-04-19T05:55:38.179837Z","shell.execute_reply":"2024-04-19T05:55:38.185263Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"N = 546\n\nwith open(reviews_path, 'r') as f:\n    data = [next(f) for x in range(N)]\n    \nwith open(labels_path, 'r') as f:\n    y = [next(f) for x in range(N)]  \n    \ny = [s.replace('\\n', '') for s in y]\ny = [int(s) for s in y]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T05:55:38.190631Z","iopub.execute_input":"2024-04-19T05:55:38.191266Z","iopub.status.idle":"2024-04-19T05:55:38.216774Z","shell.execute_reply.started":"2024-04-19T05:55:38.191203Z","shell.execute_reply":"2024-04-19T05:55:38.215553Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS  # Import stop words\n\n# Load spaCy model \nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocess(lst):\n    \"\"\"Performs lowercasing, lemmatization, and stop word removal on a list of strings.\n\n    Args:\n        lst (list): A list of strings to preprocess.\n\n    Returns:\n        list: A list of preprocessed tokens.\n    \"\"\"\n    lst = [x.replace('\\n', '') for x in lst]\n    lower_lst = [s.lower() for s in lst]  # Lowercasing\n    doc_lst = [nlp(text) for text in lower_lst]  # Create spaCy Doc objects\n\n    # Lemmatization and stop word removal\n    filtered_lst = [[token.lemma_ for token in doc] for doc in doc_lst]\n    return filtered_lst\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T05:55:38.218765Z","iopub.execute_input":"2024-04-19T05:55:38.219286Z","iopub.status.idle":"2024-04-19T05:55:39.778317Z","shell.execute_reply.started":"2024-04-19T05:55:38.219233Z","shell.execute_reply":"2024-04-19T05:55:39.776883Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_tokens = preprocess(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T05:55:39.781327Z","iopub.execute_input":"2024-04-19T05:55:39.784465Z","iopub.status.idle":"2024-04-19T05:58:46.346991Z","shell.execute_reply.started":"2024-04-19T05:55:39.784394Z","shell.execute_reply":"2024-04-19T05:58:46.345404Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Features engineering","metadata":{}},{"cell_type":"markdown","source":"1. statistic features: tf-idf","metadata":{}},{"cell_type":"code","source":"tf_idf_uni = TfidfVectorizer(stop_words='english').fit_transform(data)\ntf_idf_bi = TfidfVectorizer(stop_words='english', ngram_range=(2,2)).fit_transform(data)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T05:58:46.349311Z","iopub.execute_input":"2024-04-19T05:58:46.349745Z","iopub.status.idle":"2024-04-19T05:58:49.955158Z","shell.execute_reply.started":"2024-04-19T05:58:46.349706Z","shell.execute_reply":"2024-04-19T05:58:49.953561Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"2. Word Embedding: Glove","metadata":{}},{"cell_type":"code","source":"import gensim.downloader as api\n\nmodel = api.load('conceptnet-numberbatch-17-06-300') \n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T05:58:49.956849Z","iopub.execute_input":"2024-04-19T05:58:49.957287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(text_vec_lst[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_vec_lst = []\ntext_vec_lst = []\n\nfor text in data_tokens:\n    for token in text:\n        try: \n            token_vec_lst.append(model[token])\n        except: \n            continue\n    \n    text_vec_lst.append(sum(token_vec_lst)/len(token_vec_lst))\n    token_vec_lst = []\n    \ndata_ft = np.array(text_vec_lst)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ft.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Models Building:\n","metadata":{}},{"cell_type":"markdown","source":"1. Logistic regression\n2. Random Forest(ensemble bagging learning)\n3. SVM","metadata":{}},{"cell_type":"code","source":"LR_classifier = LogisticRegression(random_state=0, solver='liblinear') \nRF_classifier = RandomForestClassifier(n_estimators=100, random_state=1875)\nSVM_classifier = SVC(kernel='linear', C=1.0, random_state=1875) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Criteria setting","metadata":{}},{"cell_type":"markdown","source":"1. CV for F1_score and ROC_AUC","metadata":{}},{"cell_type":"code","source":"vectorizations = []\nvectorizations.append(tf_idf_uni)\nvectorizations.append(tf_idf_bi)\nvectorizations.append(data_ft)\n\nmodels = []\nmodels.append(LR_classifier)\nmodels.append(RF_classifier)\nmodels.append(SVM_classifier)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import LogisticRegression\n\ny = y\n\n\n# Evaluate using accuracy\n# 3 vectorizations\nfor i in range(3):\n    X = vectorizations[i]\n    \n    # 3 models\n    for j in range(3):\n        model = models[j]\n        \n        # 5-fold cross-validation        \n        cv = KFold(n_splits=5, shuffle=True, random_state=42) \n\n        scores = cross_val_score(model, X, y, scoring='f1', cv=cv)\n        scores2 = cross_val_score(model, X, y, scoring='roc_auc', cv=cv)\n        print(f\"Average F1_SCORE: {scores.mean()}\\nAverage ROC_AUC_SCORE: {scores2.mean()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RESULT:\n+ tf_idf_uni as vectorization of data\n+ random_forest as model","metadata":{}}]}